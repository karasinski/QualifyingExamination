%!TEX root = ../main.tex
\documentclass[float=false, crop=false]{standalone}
\usepackage[subpreambles=true]{standalone}

\begin{document}

\section{Proposed Research}
% \subsection{Proposed Research}
We propose to run human-in-the-loop subject testing experiments to understand the effects of concurrent bandwidth feedback, and to integrate the effects of this feedback into a human performance model.
To investigate the three Aims outlined in the Problem Statement, we propose two experiments and the development of a model.
%
% \subsection{Strategy}
% We will run human subject experiments to test our hypotheses.
%
% In addition to these experiments, I have also completed numerous user tests for several products developed at NASA Ames.
%
% The proposed research includes two human subject experiments and the development of a model to predict human performance.
% Over the past few years at UC Davis and NASA Ames, I’ve spent over 300 hours subject testing with more than 100 subjects see Table~\ref{table:subjecttesting}.
\begin{description}[align=left]
\item [Experiment One] investigates if concurrent bandwidth feedback can be used to teach novice subjects to interpret depth cues in a three-axis manual tracking task.
\item [Experiment Two] investigates if concurrent bandwidth feedback can decrease the required learning time to peak performance in a simulated robotic arm track and capture task.
\item [The Model] will extend Professor Hess’ structural model of the human pilot to include the effects of concurrent bandwidth feedback.
% The outputs of this model will be compared to the results of Experiment One and Two.
\end{description}
% Experiment One investigates if concurrent bandwidth feedback can be used to teach novice subjects to interpret depth cues in a three-axis manual tracking task.
% Experiment Two investigates if concurrent bandwidth feedback can decrease the required learning time to peak performance in a simulated robotic arm track and capture task.
% The Model will extend Professor Hess’ structural model of the human pilot to include the effects of concurrent bandwidth feedback.
% The outputs of this model will be compared to the results of Experiment One and Two.

Concurrent bandwidth feedback has been used in a large variety of motor control tasks, and has generally been found to improve performance.
Until recently, however, only simple tasks such as physical movements or low-dimensional pursuit tasks have been investigated.
More recent works, including the lane-keeping task by de Groot et al., and our previous work with the SAFER task, have indicated that concurrent bandwidth feedback can also be quite effective for complex tasks.
Unlike simple tasks, in which the guidance hypothesis dominates when feedback is removed, there is some evidence that concurrent bandwidth feedback can be removed after training without a loss of performance.
The decrease in required learning time, improved performance, and decreased workload seen in the SAFER task show that concurrent bandwidth feedback may prove to be most useful very early in training when subjects are first exposed to complex, highly dynamic tasks.
% While visual concurrent bandwidth feedback has been used in a variety of tasks, no researchers have investigated its effects on a three-axis tracking task.
As concurrent bandwidth feedback can improve performance without an increase in workload, it may prove a useful technique for training other robotics tasks.

% Despite extensive previous research, to the authors' knowledge there exists no study in the literature addressing human performance or workload changes in manual tracking tasks between traditional computer monitors and mobile, augmented reality headsets.
% If operator performance while using augmented reality displays is improved--or at the very least, not degraded--then these devices could prove especially valuable in scenarios where it is impractical or otherwise difficult to provide a traditional computer interface.
% There are a variety of robotics tasks, such as pick-and-place tasks, for which performance may be improved by allowing an operator the mobility to move and view the scene from whatever position is convenient at a given time.
% Traditional robotics stations require the operator to remain in a single position, and typically only allow for several camera angles.
% Mobile augmented reality displays allow the operator to take advantage of their ability to move through the environment, without needing to manage external cameras.

There has been considerable improvement in the field of pilot modeling since McRuer's crossover model, especially with models that incorporate human physiology.
The Structural Model, in particular, has been very effective in predicting pilot performance, handling qualities, pilot-induced oscillation rating levels, and workload for a variety of system dynamics.
None of these pilot models, however, are able to include the effects of concurrent bandwidth feedback.
The performance improving effects of this feedback, seen throughout the literature, make this a compelling feature to be incorporated into a pilot model.

% \begin{table}[tb]
% \centering
% \caption{Previous Subject Testing Campaigns}
% \label{table:mcruer1974a}
% \small
% \begin{tabular}{lll}
% \toprule
% Experiment   & Subjects & Length \\
% \midrule
% Lunar Lander~\cite{Karasinski2016, Karasinski2016Masters} & 30       & 3 hours \\
% 1D Tracking~\cite{Karasinski2016Masters} & 15       & 2.5 hours \\
% SAFER~\cite{Karasinski2016Masters, Karasinski2017}         & 30       & 3 hours \\
% 3D Tracking                        & 26       & 2 hours \\
% \bottomrule
% \end{tabular}
% \label{table:subjecttesting}
% \end{table}
%
% Cool things we could do with The Model:
% \begin{itemize}
% \item Predict the optimal bandwidth provided task and controller characteristics
% \item Predict the effects of scheduled bandwidth changes across training
% \end{itemize}

% Hypotheses:
% \begin{itemize}
% \item Concurrent bandwidth feedback can improve performance in simple, yet difficult tasks.
% \item The depth cue offered by 3D augmented reality displays can improve performance.
% \item Concurrent bandwidth feedback can be used to teach depth cues.
% \item Concurrent bandwidth feedback can improve performance in robotic arm pick and place task.
% \item The effects of concurrent bandwidth feedback on human performance can be modeled.
% \end{itemize}


% Experiment 1 investigates CBF guidance display benefits (will be complete)
% Experiment 2 uses simulation of robotic arm
% Compare subjects with and without feedback in a track and capture task
% We can use this to validate the results we find in following experiment
% We can also use this to create our model and validate the predictions of the model

\subsection{Experiment One}
Experiment One investigated the effects of concurrent bandwidth feedback on performance (CBF) and augmented reality displays in a three-axis manual tracking task.
In our experiment, subjects were responsible for simultaneously completing three tracking tasks and a two-choice task.
Each axis of the tracking task was disturbed by a sum-of-sines, resulting in a random appearing signal that was difficult for the subjects to predict.
The two-choice task appeared on a screen next to the tracking task, and asked subjects to respond to either a ``LEFT'' or ``RIGHT'' command.
Subjects controlled the three-axes and responded to the two-choice task by using a Microsoft Xbox controller.
The subjects used the left joystick on the controller to control the $x$ and $y$ axis tracking tasks.
Subjects moved this joystick left and right to control the $x$ axis, and up and down to control the $y$ axis.
The subjects used the right joystick on the joystick to control the $z$ axis.
Subjects moved this joystick up and down to control the $z$ axis.
Subjects used the left and right triggers on the controller to respond to the two-choice task, using the left trigger to indicate ``LEFT'' on the two-choice task, and the right trigger to indicate ``RIGHT'' on the two-choice task.

\subsubsection{Stereoscopic Displays}
Stereoscopic displays are systems ``in which two slightly different views of a scene are provided to a viewer, one image for each eye... allow[ing] the viewer's binocular visual system to extract depth information in a scene using this disparate information''~\cite{McIntire2014}.
Without the aid of the binocular depth cue presented by stereoscopic displays, viewers are instead reliant entirely on monocular clues such relative sizing, occlusion, and motion.
One of the primary motivation for stereoscopic displays is that ``[t]he visual scene of a 3D world is a more `natural,' `ecological,' or `compatible' representation than that provided by 2D displays''~\cite{Wickens1990}.
As a result of this motivation, the effects of stereoscopic displays on human performance have been extensively studied in the literature.
Several authors have attempted to classify which types of tasks may stand to benefit~\cite{McIntire2014, Wickens1990, Wickens1989, Naikar1998, Dixon2009}.
A recent review of 184 papers, for example, suggests that 60\% of studies showed some benefit of 3D stereo displays, 15\% of tasks showed unclear or mixed benefits, and 25\% of studies showed no clear benefits~\cite{McIntire2014}.
In their review, tasks involving finding/identifying/classifying objects and tasks involving real/virtual spatial manipulations of objects benefited the most, while learning/training/planning tasks were the least likely to show a benefit.

Kim et al. also performed a quantitative evaluation of perspective and stereoscopic displays in three-axis manual tracking task~\cite{Kim1987}.
They investigated the differences between perspective and stereoscopic displays, the elevation angle, azimuth angle, and the effects of two visual enhancements: a grid and a reference line.
They found very strong relationships between elevation and azimuth angles and tracking performance, with the best performance occurring with an elevation angle of 45 degrees and an azimuth angle of 0 degrees.
Tracking performance decreased rapidly as the azimuth angle varied, and decreased less rapidly as the elevation angle varied.
In general, they found that the stereoscopic display allowed for better tracking performance, though the inclusion of the reference line visual enhancement greatly decreased the benefit over the perspective display.
Using only two subjects, they provided some insight into intrasubject and intersubject variability.
In several instances, intrasubject variability showed 50\% changes within the same experimental condition, while intersubject variability also appeared quite large in some conditions.
Kim et al. repeated the evaluation of these parameters on a telerobotics pick and place study~\cite{WonKim1987}.
They found similar results in this second study, suggesting that their results could be generalized and that three-axis tracking performance can be correlated with pick and place completion time.

Smallman at al. similarly investigated the effect of visual enhancements and 2D vs 3D displays for the development of a naval air warfare console~\cite{Smallman2000}.
Participants viewed naval and aircraft tracks in either a conventional 2D top-down display or a 3D display, and then attempted to reconstruct track positions.
They investigated the effectiveness of drop-lines and drop-shadows, and found that they significantly improved subjects ability to localize aircraft compared to when the enhancements were not present.
Furthermore, in the absence of either visual enhancement subjects performed better with the 2D display than the 3D display.
Similar to Kim et al., they ultimately recommended that 3D stereoscopic displays include the use of a reference or drop-line for optimal performance.

\subsubsection{Hypotheses}
This study assessed the influence of display (perspective vs. stereoscopic), relative display attitude (zero degrees vs. thirty degrees), and concurrent bandwidth feedback (with vs. without) on performance and workload.
Objective performance was measured using the root-mean-square error (RMSE) of each of the three axes individually and combined, and subjective performance was measured with the use of a questionnaire.
Objective workload was measured using the response time to the secondary task, and subjective workload was measured using the NASA-TLX.
It was hypothesized that:
\begin{description}[align=left]
\item [Hypothesis 1] Concurrent bandwidth feedback will improve performance in the depth ($z$) axis for both display types, and will decrease workload.
\item [Hypothesis 2] Stereoscopic augmented reality displays improve performance in the depth ($z$) axis, but do not affect workload.
\item [Hypothesis 3] Rotating the display improves performance in the depth ($z$) axis for both display types, and will decrease workload.
% \item [Hypothesis 4] The performance along the $x$ and $y$ axes will be the same for all three designs.
\end{description}

\subsubsection{Procedure}
% \subsubsection{Participants}
A total of 24 subjects (19 males, 5 females) were recruited in accordance with the University of California, Davis Internal Review Board (IRB).
There were 12 subjects in the LCD group, and 12 subjects in the HoloLens group.
Subjects were undergraduate and graduate students in the University of California, Davis College of Engineering.
%, and had a mean age of $XX \pm XX$.
This experiment was approved by the Institutional Review Board at the University of California, Davis, and subjects signed a consent form and were not compensated.

% \subsubsection{Apparatus}
\begin{figure}[tb!]
    \begin{center}
        \begin{subfigure}{0.49\textwidth}
            \includegraphics[width=\linewidth]{../img/DSC_0801.JPG}
            \caption{LCD Group}
        \end{subfigure}\hfill
        \begin{subfigure}{0.49\textwidth}
            \includegraphics[width=\linewidth]{../img/DSC_0803.JPG}
            \caption{HoloLens Group}
        \end{subfigure}
    \caption{The fixed-based simulator used by both groups.}%
    \label{fig:simulator}%
    \end{center}
\end{figure}

% \paragraph{Equipment}
A human-in-the-loop simulation was conducted using a fixed-base simulator, see Figure~\ref{fig:simulator}.
The simulator consisted of two 10.4 inch LCD displays.
The tracking task was completed on the left display, while the right display showed the two-choice task.
Subjects were seated for the duration of the experiment, and were placed one meter perpendicular from the center of the left display.
For subjects in the HoloLens group, the left LCD monitor was turned off, and the tracking task was instead displayed on the HoloLens.
For these subjects, the cross was placed at the same height as the subject's head, such that it was viewed directly on in the baseline condition.
% different height subjects -> tracking cross was at different heights
% no way to avoid this AND keep the relative view angle the same between groups
% The cross was rendered at 800x480 pixels for the LCD group, and at 1268x720 pixels for the HoloLens group.
Both groups viewed the guidance cross of a width and height of 5 inches by 5 inches.
For subjects in the HoloLen's group, the $z$ motion of the cross also could move up to 5 inches in either direction away from the center of the guidance cross.
Subjects in both groups used the same Microsoft Xbox controller and control scheme to complete the task.

% \paragraph{Selection of Disturbance}
The disturbance function was a sum of 13 sinusoids approximating a rectangular spectrum with a 2.0 rad/s cutoff frequency.
The disturbing force, as a function of time, $d(t)$, was
\begin{align}
d(t) = \sum_{i=1}^{13} A_i \sin \left( w_i t + \phi_i \right)
\label{eq:disturbance}
\end{align}
The sine wave amplitude, frequency, and phase offset for each sine wave in the disturbance force was borrowed from a similar experiment~\cite{hess1984effects}.
% Table~\ref{sine-table} lists the sine wave amplitude, frequency, number of cycles in a 60 second run, and phase offset for each sine wave in the disturbance force.
This disturbance was the same for all subjects and trials, though the subjects were naive to this.
The $x$, $y$ and $z$ axes all experienced the same disturbance force generating function, but the $y$ axis was temporally offset by 60 seconds and the $z$ axis was offset by 120 seconds.
This allowed for a very similar generation of disturbance forces for each axis.
The RMSE of the disturbance force was normalized along each axis such that all three were the same.

% \begin{table}[tb]
% \caption{The relative amplitude, frequency, number of cycles in each 60 second run, and phase offset each $i^{th}$ sine, see Equation~\ref{eq:disturbance}.}
% \centering
% \begin{tabular}{*{5}{r}}
% \toprule
% i & $A_i/A_1$ & $w_i$ rad/s & \multicolumn{1}{p{3cm}}{\centering No. of cycles \\in 60-s run} & $\phi_i$ \\
% \midrule
% 1  & 1.0 &  0.18850 & 1   & $\pi/6.5$ \\
% 2  & 1.0 &  0.31416 & 3   & $2 \times \phi_1$ \\
% 3  & 1.0 &  0.50265 & 4   & $3 \times \phi_1$ \\
% 4  & 1.0 &  0.87965 & 8   & $4 \times \phi_1$ \\
% 5  & 1.0 &  1.44513 & 13  & $5 \times \phi_1$ \\
% 6  & 1.0 &  2.13628 & 20  & $6 \times \phi_1$ \\
% 7  & 0.1 &  3.07876 & 29  & $7 \times \phi_1$ \\
% 8  & 0.1 &  4.20973 & 40  & $8 \times \phi_1$ \\
% 9  & 0.1 &  5.78053 & 55  & $9 \times \phi_1$ \\
% 10 & 0.1 &  8.23097 & 78  & $10 \times \phi_1$ \\
% 11 & 0.1 & 11.24690 & 107 & $11 \times \phi_1$ \\
% 12 & 0.1 & 15.77079 & 150 & $12 \times \phi_1$ \\
% 13 & 0.1 & 23.93894 & 228 & $13 \times \phi_1$ \\
% \bottomrule
% \end{tabular}
% \label{sine-table}
% \end{table}

% \paragraph{The Three Designs}
\begin{figure}[tb!]
    \begin{center}
        \begin{subfigure}{0.32\textwidth}
            \includegraphics[trim={5cm 0 5cm 0},clip,width=\linewidth]{../img/Baseline.png}
            \caption{Baseline}
        \end{subfigure}\hfill
        \begin{subfigure}{0.32\textwidth}
            \includegraphics[trim={5cm 0 5cm 0},clip,width=\linewidth]{../img/Color.png}
            \caption{Color}
        \end{subfigure}\hfill
        \begin{subfigure}{0.32\textwidth}
            \includegraphics[trim={5cm 0 5cm 0},clip,width=\linewidth]{../img/Angled.png}
            \caption{Rotated}
        \end{subfigure}
        \caption{The three different designs in the same error state.
        (b) The color feedback has been activated here, changing the guidance target from green to red.}
        \label{fig:designs}%
    \end{center}
\end{figure}

\begin{figure}[tb!]
    \begin{center}
        % Thanks
        % https://tex.stackexchange.com/questions/67573/tikz-shift-and-rotate-in-3d

        \newcommand{\rotateRPY}[3]% roll, pitch, yaw
        {   \pgfmathsetmacro{\rollangle}{#1}
            \pgfmathsetmacro{\pitchangle}{#2}
            \pgfmathsetmacro{\yawangle}{#3}

            % to what vector is the x unit vector transformed, and which 2D vector is this?
            \pgfmathsetmacro{\newxx}{cos(\yawangle)*cos(\pitchangle)}
            \pgfmathsetmacro{\newxy}{sin(\yawangle)*cos(\pitchangle)}
            \pgfmathsetmacro{\newxz}{-sin(\pitchangle)}
            \path (\newxx,\newxy,\newxz);
            \pgfgetlastxy{\nxx}{\nxy};

            % to what vector is the y unit vector transformed, and which 2D vector is this?
            \pgfmathsetmacro{\newyx}{cos(\yawangle)*sin(\pitchangle)*sin(\rollangle)-sin(\yawangle)*cos(\rollangle)}
            \pgfmathsetmacro{\newyy}{sin(\yawangle)*sin(\pitchangle)*sin(\rollangle)+ cos(\yawangle)*cos(\rollangle)}
            \pgfmathsetmacro{\newyz}{cos(\pitchangle)*sin(\rollangle)}
            \path (\newyx,\newyy,\newyz);
            \pgfgetlastxy{\nyx}{\nyy};

            % to what vector is the z unit vector transformed, and which 2D vector is this?
            \pgfmathsetmacro{\newzx}{cos(\yawangle)*sin(\pitchangle)*cos(\rollangle)+ sin(\yawangle)*sin(\rollangle)}
            \pgfmathsetmacro{\newzy}{sin(\yawangle)*sin(\pitchangle)*cos(\rollangle)-cos(\yawangle)*sin(\rollangle)}
            \pgfmathsetmacro{\newzz}{cos(\pitchangle)*cos(\rollangle)}
            \path (\newzx,\newzy,\newzz);
            \pgfgetlastxy{\nzx}{\nzy};
        }

        \tikzset{RPY/.style={x={(\nxx,\nxy)},y={(\nyx,\nyy)},z={(\nzx,\nzy)}}}

        \begin{tikzpicture}
            \draw[-] node at (3.5,0,0) {x} (-3,0,0) -- (3,0,0);
            \draw[-] node at (0,3.5,0) {y, {\color{red} y\textprime}} (0,-3,0) -- (0,3,0);
            \draw[-] node at (0,0,3.5) {z} (0,0,-3) -- (0,0,3);

            \rotateRPY{0}{-30}{0}
            \begin{scope}[draw=red, text=red,fill=red,densely dashed,RPY]
                \draw[-] node at (3.5,0,0) {x\textprime} (-3,0,0) -- (3,0,0);
                %\draw[-] node at (0,3.5,0) {} (0,0,0) -- (0,3,0);
                \draw[-] node at (0,0,3.5) {z\textprime} (0,0,-3) -- (0,0,3);
            \end{scope}
            \draw [->] (0:1.5) arc (0:-15:1.5);
            \draw[-] node at (1.7,-.2,0) {$\theta$};
        \end{tikzpicture}

        \caption{Perspective display of the coordinate frame for the tracking tasks, with the $x$, $y$, and $z$ axes labeled. After rotating by $\theta$ around the y axis, the resulting reference frame of $x\textprime y\textprime z\textprime$ is also labeled.}
    \end{center}
\end{figure}

% \begin{table}[tb]
% \caption{The factors that were modified between the different designs.}
% \centering
% \begin{tabular}{*{3}{r}}
% \toprule
% Design & $\theta$ (degrees) & Feedback \\
% \midrule
% Baseline & 0 & No \\
% Feedback & 0 & Yes \\
% Rotated & 30 & No \\
% \bottomrule
% \end{tabular}
% \label{tab:designs}
% \end{table}

Three designs were presented to the subjects to evaluate: a baseline design, a color-based concurrent bandwidth feedback (CBF) design, and a rotated design.
Figure~\ref{fig:designs} shows all three designs in the same error state.
The three designs were very similar, having only minor differences between each other.
Compared to the baseline, the color feedback and rotated displays were expected to produce superior performance along the $z$ axis tracking task.

The baseline design consists of a flat cross with a center target point and a green sphere error indicator.
This indicator also casts a green rod perpendicular to the plane of the cross, which allows for a visual estimation of the error in the $z$ axis.
The $x$ axis is parallel with the horizontal cross, while the $y$ axis is parallel with the vertical cross.
The color feedback display was identical to the baseline design in every way, but also had visual concurrent bandwidth feedback on the $z$ axis.
When the absolute value of the error on the $z$ axis grew over a fixed bandwidth of 0.2 units, the color of both the spherical indicator and the cylindrical rod changed from green to red.
When the absolute value of the error on the $z$ axis was lowered back below this fixed bandwidth, the indicator changed back to a green color.
The rotated display was identical to the baseline design, but the relative attitude of the display was rotated about the $y$ axis by 30 degrees.

Before entering the study, subjects were randomly placed in a display group (either the LCD monitor or HoloLens), and were then randomly placed into an order group (which consisted of Baseline-Feedback-Rotated, Feedback-Rotated-Baseline, and Rotated-Baseline-Feedback).
This order group was created to remove any order effects that might arise due to training on a given display, and follows a standard Latin squares design.
(The order of the designs was initially considered to be insignificant, but we will later discuss how this is not the case.)
% Subjects were naive to this order, and were only told which design they were evaluating directly before they began.
After entering the experiment room, the subjects signed a consent form, and then sat through a twenty minute training session which familiarized them with the task, the three designs, the NASA-TLX, and the controller.
Subjects were instructed that they were evaluating three different display designs. %: a baseline display, a color feedback display, and and rotated display.
They were further instructed that they had two tasks.
They were told to:
\begin{itemize}
    \item Minimize the displacement of their guidance target from the center
    \item Respond to the two choice task as accurately and quickly as possible
\end{itemize}

After the training, subjects were seated in front of the simulator.
Subjects in the HoloLens group then completed a short calibration program that adjusted the display to their interpupillary distance.
After this, they were instructed how to align the guidance cross with the center of the left display monitor.
All subjects were then allowed to complete two familiarization trials, during which time they could ask questions about how the controls worked, or any other aspects of the task.
The proctor also used this time to ensure that subjects had a basic grasp of the task, and were responding to both the tracking and two-choice tasks appropriately.
All familiarizations were done with the baseline design, regardless of which design the subjects evaluated first.

After this familiarization process, subjects completed ten trials in their first design.
Finishing this, they then completed a brief survey which asked them if the design was adequate to complete the task.
Subjects were also asked to subjectively rate their performance in a questionnaire after evaluating each design.
Subjects were asked ``I found the tracking display adequate to complete the task.'', and were asked to respond on a five point scale where 1 indicated ``Strongly Disagree'' and 5 indicated ``Strongly Agree''.
After this survey, subjects then completed a NASA-TLX workload survey.
Subjects then repeated this process for their second and third designs.
After completing this process for their third design, subjects were also asked to complete a preference survey which inquired into what design the subjects believed to be the best.

\subsubsection{Results}
We conducted three-way mixed ANOVAs between device (HoloLens or LCD), design (Baseline, Feedback, or Rotated), and starting design (Baseline, Feedback, or Rotated) with repeated measures on the design factor.
When significant effects were observed, post hoc comparisons using the Tukey HSD test with a Bonferroni adjustment were completed to investigate which pairs of the factor were significant.
In order to remove learning and fatigue effects, each subject's best performing five trials in each design were averaged together to produce one average score for each subject and design.
Additionally, the first ten seconds of each sixty second trial were not included in the analysis to remove initial transient effects.

% \subsection{Tracking Performance}
The root-mean-square error (RMSE) of each axis was analyzed individually and combined to understand the differences between the three designs and the two devices.
% For a given variable, $i$, RMSE is calculated via
% \begin{align}
% \text{RMSE}_i = \sqrt{\dfrac{1}{n} \sum_{j=1}^{n} \left(i_j - \hat{i}_j\right)^{2}}
% \end{align}
% where $i_j$ is an observed value at time $j$ and $\hat{i}_j$ is a targeted, predicted, or otherwise desired value.
% As this is a regulation task, however, $\hat{i}_j = 0$ for all $j$.
The RMS of the disturbance signal was calculated along each axis, and used to normalize the RMSE.
As such, an RMSE of 1 indicates performance that is just as equivalent as inserting no input, and an RMSE greater than 1 indicates quite poor performance.
% The RMSE along the $i^{th}$ axis, RMSE$_i$, was calculated for the $x$, $y$, and $z$ axes via
% \begin{align}
% \text{RMSE}_i = \sqrt{\dfrac{1}{n} \sum_{j=1}^{n} \left(i_j\right)^{2}}
% \end{align}
It was expected that the baseline design would lead to the worst performance in the $z$ axis, while the color feedback and rotated designs would have better performance in the $z$ axis.
It was also expected that, due to the stereoscopic nature of the display, the HoloLens would allow for better performance than the LCD monitor along $z$ axis.

% \begin{table}[tb]
% \caption{Results of three-way ANOVA on $z$ axis RMSE.
% All main effects significant, several significant interactions.}
% \centering
% \begin{tabular}{*{5}{r}}
% \toprule
%                             & numDF & denDF & F-value   & p-value \\
% \midrule
% % (Intercept)                 & 1     & 36     & 2354.6349 & <.0001 \\
% design                      & 2     & 36     &   84.9179 & <.0001 \\
% device                      & 1     & 18     &    7.2158 & 0.0151 \\
% startdesign                 & 2     & 18     &    4.8132 & 0.0212 \\
% design:device               & 2     & 36     &    1.8359 & 0.1741 \\
% design:startdesign          & 4     & 36     &    8.5499 & 0.0001 \\
% device:startdesign          & 2     & 18     &    2.4652 & 0.1132 \\
% design:device:startdesign   & 4     & 36     &    5.5672 & 0.0014 \\
% \bottomrule
% \end{tabular}
% \label{tab:exp1anovarmse}
% \end{table}

% \begin{table}[tb]
% \caption{Results of three-way ANOVA on NASA-TLX.
% No significant effects or interactions.}
% \centering
% \begin{tabular}{*{5}{r}}
% \toprule
%                             & numDF & denDF & F-value   & p-value \\
% \midrule
% % (Intercept)                 & 1   & 36 & 472.0358 & <.0001 \\
% design                      & 2   & 36 &   0.3615 & 0.6991 \\
% device                      & 1   & 18 &   0.3797 & 0.5455 \\
% startdesign                 & 2   & 18 &   0.2769 & 0.7613 \\
% design:device               & 2   & 36 &   0.5595 & 0.5764 \\
% design:startdesign          & 4   & 36 &   0.2649 & 0.8985 \\
% device:startdesign          & 2   & 18 &   0.0025 & 0.9975 \\
% design:device:startdesign   & 4   & 36 &   0.5381 & 0.7086 \\
% \bottomrule
% \end{tabular}
% \label{tab:exp1anovaworkload}
% \end{table}

% > startdesign
%          Simultaneous Tests for General Linear Hypotheses
% Multiple Comparisons of Means: Tukey Contrasts
% Fit: lme.formula(fixed = Zrmse ~ design * device * startdesign, data = data,
%     random = ~1 | subject/design)

% Linear Hypotheses:
%            Estimate Std. Error z value Pr(>|z|)
% 2 - 1 == 0 -0.27861    0.07566  -3.683 0.000693 ***
% 3 - 1 == 0  0.11523    0.07566   1.523 0.383277
% 3 - 2 == 0  0.39383    0.07566   5.206  5.8e-07 ***
% ---
% Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
% (Adjusted p values reported -- bonferroni method)

\begin{figure}[tb!]
    \begin{center}
        \includegraphics[width=\linewidth]{./../img/x_design_y_zrmse_col_startdesign_hue_device.pdf}
        % \includegraphics[width=\linewidth]{./../img/x_design_y_zrmse_hue_startdesign_col_device.pdf}
        \caption{Three-way ANOVA $z$-axis RMSE. Additional formatting required.}
        \label{fig:zrmseanovas}
    \end{center}
\end{figure}

Results of the ANOVA on the $z$-axis RMSE showed significant effects for design ($F(2, 36)=84.9, p<.001$), device ($F(1, 18)= 7.2, p<0.015$), and start design ($F(2, 18)=4.8, p<0.021$).
The ANOVA also showed a significant interaction effect between design and starting design ($F(4, 36)=8.5499, p<0.0001$), and a three way interaction between design, device, and starting design ($F(4, 36)=5.5672, p<0.0014$).
Further investigating the effect of starting design, a Tukey Honest Significance Difference (HSD) comparison with a Bonferroni correction shows significance differences between subjects that started in the concurrent bandwidth feedback group and those in the baseline ($p<0.001$) or rotated ($p<0.001$) designs, but no difference between subjects that started in the baseline and rotated designs ($p>.38$).
The difference in performance along the depth axis between design, device, and starting design can be seen in Figure~\ref{fig:zrmseanovas}.

% >     y = lme(Zrmse ~ design*device, data=data, random=~1|subject/design, subset=startdesign==2)
% >     anova <- anova(y)
% > anova
%               numDF denDF   F-value p-value
% (Intercept)       1    12 2665.7021  <.0001
% design            2    12   21.6481  0.0001
% device            1     6   36.7320  0.0009
% design:device     2    12    5.4211  0.0210
%
% >     y = lme(Zrmse ~ design*device, data=data, random=~1|subject/design, subset=startdesign!=2)
% >     anova <- anova(y)
% > anova
%               numDF denDF   F-value p-value
% (Intercept)       1    28 1285.5677  <.0001
% design            2    28   38.6980  <.0001
% device            1    14    1.0299  0.3274
% design:device     2    28    0.0263  0.9741

Due to the presence of interaction effects in the starting design factor, the remainder of the analysis is split between subjects who started with the concurrent bandwidth feedback design and those that did not (e.g., those that started in the baseline or rotated design).
For subjects that started in the CBF design, there was a significant effect of design ($F(2, 12)= 21.6481, p <.0001$), a significant effect or display ($F(1, 6)=  36.7320, p <0.001$), and a significant interaction effect between design and device ($F(2, 12)=  5.4211, p <0.021$).
For subjects that did not start in the CBF design, there was a significant effect of design ($F(2, 28)= 38.6980, p <.0001$), no significant effect or display ($F(1, 14)=  1.0299, p >0.32$), and no interaction effect ($F(2, 12)=  0.0263, p >0.97$).
The resulting difference between subjects who started in the CBF design and those that did not is presented in Figure~\ref{fig:zrmseanovas2}.

\begin{figure}[tb!]
    \begin{center}
        \includegraphics[width=\linewidth]{./../img/x_design_y_zrmse_hue_device_col_cbf_first.pdf}
        \caption{Three-way ANOVA $z$-axis RMSE. Additional formatting required. Difference between starting with/without CBF.}
        \label{fig:zrmseanovas2}
    \end{center}
\end{figure}

% >     y = lme(rt ~ design*device*startdesign, data=data, random=~1|subject/design)
% >     anova <- anova(y)
% > anova
%                           numDF denDF   F-value p-value
% (Intercept)                   1    36 1290.3273  <.0001
% design                        2    36    7.9333  0.0014
% device                        1    18    1.2139  0.2851
% startdesign                   2    18    2.0733  0.1548
% design:device                 2    36    0.0045  0.9955
% design:startdesign            4    36    1.1689  0.3408
% device:startdesign            2    18    1.1634  0.3348
% design:device:startdesign     4    36    0.1799  0.9473
% >     y = lme(Score ~ design*device*startdesign, data=data, random=~1|subject/design)
% >     anova <- anova(y)
% > anova
%                           numDF denDF  F-value p-value
% (Intercept)                   1    36 472.0358  <.0001
% design                        2    36   0.3615  0.6991
% device                        1    18   0.3797  0.5455
% startdesign                   2    18   0.2769  0.7613
% design:device                 2    36   0.5595  0.5764
% design:startdesign            4    36   0.2649  0.8985
% device:startdesign            2    18   0.0025  0.9975
% design:device:startdesign     4    36   0.5381  0.7086

The NASA-TLX was used to measure differences in subjective workload, and the reaction time to the secondary task was used to measure differences in objective workload between design, device, and starting design.
There were no significant effects, nor interaction effects, found in the ANOVA for design, device, or starting design for the NASA-TLX measurements.
There was a significant effect of design ($F(2, 36)=  7.9333, p <0.0014$) for the reaction time to the secondary task, though the magnitude of this effect was very small between designs, less than 100ms difference, and was not significant during Tukey HSD tests.
In general, there were no significant effects found for workload measurements.

To summarize these results, there were significant effects found in the $z$-axis RMSE for design, with subjects generally performing the best using the angled design, followed by the CBF design, and performing worst with the baseline design.
There were significant effects found for the factors of device and starting design, though these must be interpreted carefully.
Subjects that started in the design with concurrent bandwidth feedback performed better than subjects that did not.
We believe that this result follows that found in our prior SAFER experiment, where subjects that were exposed to the CBF early on learned the task better than those that were not exposed.
An interesting effect of this exposure is that, after learning the task with CBF, subjects continued on to perform significantly better in the baseline condition than those subjects that did not start in the CBF design.

Subjects that started in the CBF design and that were wearing the HoloLens appear to have used the CBF to better learn the depth cue presented in the stereoscopic display.
These subjects continued to perform significantly better than subjects who started with the CBF design but without the stereoscopic display when they continued to the baseline design.
This indicates that even a brief exposure to the concurrent bandwidth feedback was sufficient to induce improved performance.
Additionally, this also suggests that well trained subjects could perform better using the stereoscopic display compared with the traditional display, but that subjects who were still learning the task could not take advantage of the additional depth cues provided by the display.
Finally, there were no significant effects found for the NASA-TLX measurements, and there were significant effects found between the designs for reaction time, though this difference was very small at less than 100ms.

% Returning to our hypotheses, we find that:
% \begin{description}[align=left]
% \item [Hypothesis 1] Concurrent bandwidth feedback will improve performance in the depth ($z$) axis for both display types, and will decrease workload.
% \item [Hypothesis 2] Stereoscopic augmented reality displays improve performance in the depth ($z$) axis, but do not affect workload.
% \item [Hypothesis 3] Rotating the display improves performance in the depth ($z$) axis for both display types, and will decrease workload.
% % \item [Hypothesis 4] The performance along the $x$ and $y$ axes will be the same for all three designs.
% \end{description}

In summary, we find partial agreement with all of our hypotheses in respect to the performance aspects of our experiment, while the workload was essentially unaffected by all of our experimental factors.
% For Hypothesis 1, ``Concurrent bandwidth feedback will improve performance in the depth ($z$) axis for both display types, and will decrease workload,'' we find partial agreement.
For Hypothesis 1, subjects that completed the baseline design before the CBF design performed better in the CBF design, while subjects that completed the CBF design before the baseline performed approximately the same in both designs.
This indicates that CBF can both improve performance compared to a baseline design, and better train subjects such that, even after brief exposure, the feedback is no longer required.
Workload was unaffected by the CBF.
%
% Addressing Hypothesis 2, ``Stereoscopic augmented reality displays improve performance in the depth ($z$) axis, but do not affect workload,'' we also find partial agreement.
For Hypothesis 2, subjects that started in the CBF design appear to have better learned the task and used the CBF to learn to interpret the depth cue provided by the stereoscopic display.
Subjects that were not initially exposed to this feedback were unable to take advantage of the display, and did not perform significantly better than subjects without the display.
%
% In response to Hypothesis 3, ``Rotating the display improves performance in the depth ($z$) axis for both display types, and will decrease workload,''
For Hypothesis 3, subjects in the rotated design did perform better than those in the baseline design, and appeared to be able to use this view to better interpret the depth of the target.
An unexpected side effect of angling the display, however, was that subjects performed worse in the $x$ axis tracking task, which was rotated out of parallel with the display.

\subsection{Experiment 2}
\begin{figure}[tb]
    \begin{center}
        \includegraphics[trim={13cm 5cm 22cm 15.5cm},clip,width=\linewidth]{./../img/Screen Shot 2018-07-26 at 1.43.16 PM.png}
        \caption{The Robotic On-board Trainer (ROBoT) station set up in the NASA HERA Analog, from~\cite{robottalk}}
        \label{figure:robotinhera}
    \end{center}
\end{figure}

Experiment Two will investigate if concurrent bandwidth feedback can decrease the required learning time to peak performance in a simulated robotic arm track and capture task.
We will compare task performance and workload between two groups: a control group, which receives no feedback, and a treatment group, which receives concurrent bandwidth feedback on one or more sensor readouts.
Subjects in both groups will complete the same task, and it is hypothesized that subjects in the treatment group will perform the task better and require less training time to reach peak performance.
This hypothesis is based on the results of the SAFER experiment, as well as the results of Experiment One.

In this task, subjects will command a robotic arm to track an approaching vehicle and grapple with it.
This is motivated by the primary use of the robot arm on the International Space Station, which grasps visiting vehicles when they arrive at the station and then attaches them to a separate fixed location on the station.
Subjects will be trained on this task, and will then repeat the task for one to two hours through a variety of slightly different start and end conditions.
% While they are completing the robotics task, subjects will also be required to attend to a secondary task which will require them to look away from their primary flight display.
% Subjects will also be asked to report their subjective workload after each trial.

NASA’s Robotic On-board Trainer (ROBoT) will be used for this experiment.
ROBoT is a package of simulation software which includes a dynamic model of the robotic arm on the space station, and presents the user with multiple camera angle views and the instrument panel required to operate effectively, see Figure~\ref{figure:robotinhera}.
In addition to these displays, ROBoT also includes the two hand controllers required to control the arm.
NASA trainers at Johnson Space Center developed metrics which are included in ROBoT, and include the time to capture, alignment measurements during approach, the amount of wobble in the arm, the number of times the grapple fixture contacted the structure, the overall path efficiency, and the number of capture attempts.
This presents a list of candidate metrics from which we may observe human performance.

% \begin{figure}[tb!]
%     \begin{center}
%         \includegraphics[trim={13cm 5cm 22cm 15.5cm},clip,width=\linewidth]{./../img/Screen Shot 2018-07-26 at 1.43.02 PM.png}
%         \caption{ROBoT visualization laptop, showing four camera views.}
%         % \label{}
%     \end{center}
% \end{figure}

% \begin{figure}[tb!]
%     \begin{center}
%         \includegraphics[trim={13cm 5cm 22cm 15.5cm},clip,width=\linewidth]{./../img/Screen Shot 2018-07-26 at 1.43.05 PM.png}
%         \caption{The camera attached to the end effector of the robotic arm, showing the grapple fixture.}
%         % \label{}
%     \end{center}
% \end{figure}

% \begin{figure}[tb!]
%     \begin{center}
%         \includegraphics[trim={13cm 5cm 22cm 15.5cm},clip,width=\linewidth]{./../img/Screen Shot 2018-07-26 at 1.43.07 PM.png}
%         \caption{Example performance score report shown to the user after each trial.}
%         % \label{}
%     \end{center}
% \end{figure}

% \begin{figure}[tb!]
%     \begin{center}
%         \includegraphics[trim={13cm 5cm 22cm 15.5cm},clip,width=\linewidth]{./../img/Screen Shot 2018-07-26 at 1.43.10 PM.png}
%         \caption{The hand controller inputs and position and angular errors are also logged throughout the trial.}
%         % \label{}
%     \end{center}
% \end{figure}

\subsubsection{Hypotheses}
This study will assess the influence of concurrent bandwidth feedback (with vs. without) on performance and workload.
Objective performance data will be measured using time to capture, alignment error metrics, and other metrics available by the ROBoT system.
Subjective workload will be measured using the NASA-TLX at several points throughout the experiment.
It is hypothesized that:
\begin{description}[align=left]
\item [Hypothesis 1] Concurrent bandwidth feedback will improve performance in the track-and-capture task.
\item [Hypothesis 2] Concurrent bandwidth feedback will cause subjects to more quickly reach their peak performance in the track-and-capture task.
\item [Hypothesis 3] Concurrent bandwidth feedback will decrease workload in the track-and-capture task.
\end{description}

\subsection{Model Extension}
The Model will extend Professor Hess’ structural model of the human pilot to include the effects of concurrent bandwidth feedback.
The outputs of this model will be compared to the results of Experiment One and Two.

\end{document}
