%!TEX root = ../main.tex
\documentclass[float=false, crop=false]{standalone}
\usepackage[subpreambles=true]{standalone}

\begin{document}

\section{Proposed Research}
% \subsection{Proposed Research}
We propose to run human-in-the-loop subject testing experiments to understand the effects of concurrent bandwidth feedback, and to integrate the effects of this feedback into a human performance model.
To investigate the three Aims outlined in the Problem Statement, we propose two experiments and the development of a model.
%
% \subsection{Strategy}
% We will run human subject experiments to test our hypotheses.
%
% In addition to these experiments, I have also completed numerous user tests for several products developed at NASA Ames.
%
% The proposed research includes two human subject experiments and the development of a model to predict human performance.
% Over the past few years at UC Davis and NASA Ames, I’ve spent over 300 hours subject testing with more than 100 subjects see Table~\ref{table:subjecttesting}.
\begin{description}[align=left]
\item [Experiment One] investigates if concurrent bandwidth feedback can be used to teach novice subjects to interpret depth cues in a three-axis manual tracking task.
\item [Experiment Two] investigates if concurrent bandwidth feedback can decrease the required learning time to peak performance in a simulated robotic arm track and capture task.
\item [The Model] will extend Professor Hess’ structural model of the human pilot to include the effects of concurrent bandwidth feedback.
% The outputs of this model will be compared to the results of Experiment One and Two.
\end{description}
% Experiment One investigates if concurrent bandwidth feedback can be used to teach novice subjects to interpret depth cues in a three-axis manual tracking task.
% Experiment Two investigates if concurrent bandwidth feedback can decrease the required learning time to peak performance in a simulated robotic arm track and capture task.
% The Model will extend Professor Hess’ structural model of the human pilot to include the effects of concurrent bandwidth feedback.
% The outputs of this model will be compared to the results of Experiment One and Two.

Concurrent bandwidth feedback has been used in a large variety of motor control tasks, and has generally been found to improve performance.
Until recently, however, only simple tasks such as physical movements or low-dimensional pursuit tasks have been investigated.
More recent works, including the lane-keeping task by de Groot et al., and our previous work with the SAFER task, have indicated that concurrent bandwidth feedback can also be quite effective for complex tasks.
Unlike simple tasks, in which the guidance hypothesis dominates when feedback is removed, there is some evidence that concurrent bandwidth feedback can be removed after training without a loss of performance.
The decrease in required learning time, improved performance, and decreased workload seen in the SAFER task show that concurrent bandwidth feedback may prove to be most useful very early in training when subjects are first exposed to complex, highly dynamic tasks.
% While visual concurrent bandwidth feedback has been used in a variety of tasks, no researchers have investigated its effects on a three-axis tracking task.
As concurrent bandwidth feedback can improve performance without an increase in workload, it may prove a useful technique for training other robotics tasks.

% Despite extensive previous research, to the authors' knowledge there exists no study in the literature addressing human performance or workload changes in manual tracking tasks between traditional computer monitors and mobile, augmented reality headsets.
% If operator performance while using augmented reality displays is improved--or at the very least, not degraded--then these devices could prove especially valuable in scenarios where it is impractical or otherwise difficult to provide a traditional computer interface.
% There are a variety of robotics tasks, such as pick-and-place tasks, for which performance may be improved by allowing an operator the mobility to move and view the scene from whatever position is convenient at a given time.
% Traditional robotics stations require the operator to remain in a single position, and typically only allow for several camera angles.
% Mobile augmented reality displays allow the operator to take advantage of their ability to move through the environment, without needing to manage external cameras.

There has been considerable improvement in the field of pilot modeling since McRuer's crossover model, especially with models that incorporate human physiology.
The Structural Model, in particular, has been very effective in predicting pilot performance, handling qualities, pilot-induced oscillation rating levels, and workload for a variety of system dynamics.
None of these pilot models, however, are able to include the effects of concurrent bandwidth feedback.
The performance improving effects of this feedback, seen throughout the literature, make this a compelling feature to be incorporated into a pilot model.

% \begin{table}[tb]
% \centering
% \caption{Previous Subject Testing Campaigns}
% \label{table:mcruer1974a}
% \small
% \begin{tabular}{lll}
% \toprule
% Experiment   & Subjects & Length \\
% \midrule
% Lunar Lander~\cite{Karasinski2016, Karasinski2016Masters} & 30       & 3 hours \\
% 1D Tracking~\cite{Karasinski2016Masters} & 15       & 2.5 hours \\
% SAFER~\cite{Karasinski2016Masters, Karasinski2017}         & 30       & 3 hours \\
% 3D Tracking                        & 26       & 2 hours \\
% \bottomrule
% \end{tabular}
% \label{table:subjecttesting}
% \end{table}
%
% Cool things we could do with The Model:
% \begin{itemize}
% \item Predict the optimal bandwidth provided task and controller characteristics
% \item Predict the effects of scheduled bandwidth changes across training
% \end{itemize}

% Hypotheses:
% \begin{itemize}
% \item Concurrent bandwidth feedback can improve performance in simple, yet difficult tasks.
% \item The depth cue offered by 3D augmented reality displays can improve performance.
% \item Concurrent bandwidth feedback can be used to teach depth cues.
% \item Concurrent bandwidth feedback can improve performance in robotic arm pick and place task.
% \item The effects of concurrent bandwidth feedback on human performance can be modeled.
% \end{itemize}


% Experiment 1 investigates CBF guidance display benefits (will be complete)
% Experiment 2 uses simulation of robotic arm
% Compare subjects with and without feedback in a track and capture task
% We can use this to validate the results we find in following experiment
% We can also use this to create our model and validate the predictions of the model

\subsection{Experiment One}
Experiment One investigated the effects of concurrent bandwidth feedback (CBF) and augmented reality displays on performance and workload in a three-axis manual tracking task.
In our experiment, subjects were responsible for simultaneously completing three tracking tasks and a two-choice task.
Each axis of the tracking task was disturbed by a sum-of-sines, resulting in an appearingly random signal that was difficult for the subjects to predict.
The two-choice task appeared on a screen next to the tracking task and prompted subjects to respond to either a ``LEFT'' or ``RIGHT'' command.
Subjects controlled the three-axes and responded to the two-choice task by using a Microsoft Xbox controller.
The subjects used the left joystick on the controller to control the $x$ and $y$ axis tracking tasks.
Subjects moved this joystick left and right to control the $x$ axis, and up and down to control the $y$ axis.
The subjects controlled the $z$ axis by moving the right joystick up and down.
Subjects used the left and right triggers on the controller to respond to the two-choice task, using the left trigger to indicate ``LEFT'', and the right trigger to indicate ``RIGHT''.

\subsubsection{Stereoscopic Displays}
Stereoscopic displays are systems ``in which two slightly different views of a scene are provided to a viewer, one image for each eye... allow[ing] the viewer's binocular visual system to extract depth information in a scene using this disparate information''~\cite{McIntire2014}.
Without the aid of the binocular depth cue presented by stereoscopic displays, viewers are instead reliant entirely on monocular clues such relative sizing, occlusion, and motion.
One of the primary motivation for stereoscopic displays is that ``[t]he visual scene of a 3D world is a more `natural,' `ecological,' or `compatible' representation than that provided by 2D displays''~\cite{Wickens1990}.
As a result of this motivation, the effects of stereoscopic displays on human performance have been extensively studied in the literature.
Several authors have attempted to classify which types of tasks may stand to benefit~\cite{McIntire2014, Wickens1990, Wickens1989, Naikar1998, Dixon2009}.
A recent review of 184 papers, for example, suggests that 60\% of studies showed some benefit of 3D stereo displays, 15\% of tasks showed unclear or mixed benefits, and 25\% of studies showed no clear benefits~\cite{McIntire2014}.
In their review, tasks involving finding/identifying/classifying objects and tasks involving real/virtual spatial manipulations of objects benefited the most, while learning/training/planning tasks were the least likely to show a benefit.

Kim et al. also performed a quantitative evaluation of perspective and stereoscopic displays in three-axis manual tracking task~\cite{Kim1987}.
They investigated the differences between perspective and stereoscopic displays, the elevation angle, azimuth angle, and the effects of two visual enhancements: a grid and a reference line.
They found very strong relationships between elevation and azimuth angles and tracking performance, with the best performance occurring at an elevation angle of 45 degrees and an azimuth angle of 0 degrees.
Tracking performance decreased rapidly as the azimuth angle varied, and decreased less rapidly as the elevation angle varied.
In general, they found that the stereoscopic display allowed for better tracking performance, though the inclusion of the reference line visual enhancement greatly decreased the benefit over the perspective display.
Using only two subjects, they provided some insight into intrasubject and intersubject variability.
In several instances, intrasubject variability showed 50\% changes within the same experimental condition, while intersubject variability also appeared quite large in some conditions.
Kim et al. repeated the evaluation of these parameters on a telerobotics pick and place study~\cite{WonKim1987}.
They found similar results in this second study, suggesting that their results could be generalized and that three-axis tracking performance can be correlated with pick and place completion time.

Smallman at al. similarly investigated the effect of visual enhancements and 2D vs 3D displays for the development of a naval air warfare console~\cite{Smallman2000}.
Participants viewed naval and aircraft tracks in either a conventional 2D top-down display or a 3D display, and then attempted to reconstruct track positions.
They investigated the effectiveness of drop-lines and drop-shadows, and found that they significantly improved subjects ability to localize aircraft compared to when the enhancements were not present.
Furthermore, in the absence of either visual enhancement, subjects performed better with the 2D display than the 3D display.
Similar to Kim et al., they ultimately recommended that 3D stereoscopic displays include the use of a reference or drop-line for optimal performance.

\subsubsection{Hypotheses}
This study assessed the influence of display type (perspective vs. stereoscopic), relative display attitude (zero degrees vs. thirty degrees), and concurrent bandwidth feedback (with vs. without) on performance and workload.
Objective performance was measured using the root-mean-square error (RMSE) of each of the three axes individually and combined, and subjective performance was measured with the use of a questionnaire.
Objective workload was measured using the response time to the secondary task, and subjective workload was measured using the NASA-TLX.
It was hypothesized that:
\begin{description}[align=left]
\item [Hypothesis 1] Concurrent bandwidth feedback will improve performance in the depth ($z$) axis for both display types, and will decrease workload.
\item [Hypothesis 2] Stereoscopic augmented reality displays improve performance in the depth ($z$) axis, but do not affect workload.
\item [Hypothesis 3] Rotating the display improves performance in the depth ($z$) axis for both display types, and will decrease workload.
% \item [Hypothesis 4] The performance along the $x$ and $y$ axes will be the same for all three designs.
\end{description}

\subsubsection{Procedure}
% \subsubsection{Participants}
A total of 24 subjects (19 males, 5 females) were recruited in accordance with the University of California, Davis Internal Review Board (IRB), and subjects signed a consent form and were not compensated.
There were 12 subjects in the 2D group, and 12 subjects in the 3D group.
Subjects were undergraduate and graduate students in the University of California, Davis College of Engineering.
All participating subjects had normal vision (no colorblindness, eyesight correctable to 20/20 vision) and full motor control of their hands. 

% \subsubsection{Apparatus}
\begin{figure}[tb!]
    \begin{center}
        \begin{subfigure}{0.49\textwidth}
            \includegraphics[width=\linewidth]{../img/DSC_0801.JPG}
            \caption{2D Group}
        \end{subfigure}\hfill
        \begin{subfigure}{0.49\textwidth}
            \includegraphics[width=\linewidth]{../img/DSC_0803.JPG}
            \caption{3D Group}
        \end{subfigure}
    \caption{The fixed-based simulator used by both groups.}%
    \label{fig:simulator}%
    \end{center}
\end{figure}

% \paragraph{Equipment}
A human-in-the-loop simulation was conducted using a fixed-base simulator, see Figure~\ref{fig:simulator}.
The simulator consisted of two 10.4 inch LCD displays.
The tracking task was completed on the left display, while the right display showed the two-choice task.
Subjects were seated for the duration of the experiment and were placed one meter perpendicular from the center of the left display.
For subjects in the 3D group, the left LCD monitor was turned off, and the tracking task was instead displayed on the HoloLens.
For these subjects, the cross was placed at the same height as the subject's head, such that it was viewed with no relative attitude in the baseline condition.
% different height subjects -> tracking cross was at different heights
% no way to avoid this AND keep the relative view angle the same between groups
% The cross was rendered at 800x480 pixels for the LCD group, and at 1268x720 pixels for the 3D group.
Both groups viewed the guidance cross of a width and height of 5 inches by 5 inches.
For subjects in the HoloLen's group, the $z$ motion of the cross also could move up to 5 inches in either direction away from the center of the guidance cross.
% Subjects in both groups used the same Microsoft Xbox controller and control scheme to complete the task.

% \paragraph{Selection of Disturbance}
The disturbance function was a sum of 13 sinusoids approximating a rectangular spectrum with a 2.0 rad/s cutoff frequency.
The disturbing force, as a function of time, $d(t)$, was
\begin{align}
d(t) = \sum_{i=1}^{13} A_i \sin \left( w_i t + \phi_i \right)
\label{eq:disturbance}
\end{align}
Each sine wave amplitude, frequency, and phase offset was borrowed from a similar experiment~\cite{hess1984effects}.
% Table~\ref{sine-table} lists the sine wave amplitude, frequency, number of cycles in a 60 second run, and phase offset for each sine wave in the disturbance force.
This disturbance was the same for all subjects and trials, though the subjects were naive to this.
The $x$, $y$, and $z$ axes all experienced the same disturbance force generating function, but the $y$ axis was temporally offset by 60 seconds and the $z$ axis was offset by 120 seconds.
This allowed for a very similar generation of disturbance forces for each axis.
The RMSE of the disturbance force was normalized along each axis such that all three were the same.

% \begin{table}[tb]
% \caption{The relative amplitude, frequency, number of cycles in each 60 second run, and phase offset each $i^{th}$ sine, see Equation~\ref{eq:disturbance}.}
% \centering
% \begin{tabular}{*{5}{r}}
% \toprule
% i & $A_i/A_1$ & $w_i$ rad/s & \multicolumn{1}{p{3cm}}{\centering No. of cycles \\in 60-s run} & $\phi_i$ \\
% \midrule
% 1  & 1.0 &  0.18850 & 1   & $\pi/6.5$ \\
% 2  & 1.0 &  0.31416 & 3   & $2 \times \phi_1$ \\
% 3  & 1.0 &  0.50265 & 4   & $3 \times \phi_1$ \\
% 4  & 1.0 &  0.87965 & 8   & $4 \times \phi_1$ \\
% 5  & 1.0 &  1.44513 & 13  & $5 \times \phi_1$ \\
% 6  & 1.0 &  2.13628 & 20  & $6 \times \phi_1$ \\
% 7  & 0.1 &  3.07876 & 29  & $7 \times \phi_1$ \\
% 8  & 0.1 &  4.20973 & 40  & $8 \times \phi_1$ \\
% 9  & 0.1 &  5.78053 & 55  & $9 \times \phi_1$ \\
% 10 & 0.1 &  8.23097 & 78  & $10 \times \phi_1$ \\
% 11 & 0.1 & 11.24690 & 107 & $11 \times \phi_1$ \\
% 12 & 0.1 & 15.77079 & 150 & $12 \times \phi_1$ \\
% 13 & 0.1 & 23.93894 & 228 & $13 \times \phi_1$ \\
% \bottomrule
% \end{tabular}
% \label{sine-table}
% \end{table}

% \paragraph{The Three Designs}
\begin{figure}[tb!]
    \begin{center}
        \begin{subfigure}{0.32\textwidth}
            \includegraphics[trim={5cm 0 5cm 0},clip,width=\linewidth]{../img/Baseline.png}
            \caption{Baseline}
        \end{subfigure}\hfill
        \begin{subfigure}{0.32\textwidth}
            \includegraphics[trim={5cm 0 5cm 0},clip,width=\linewidth]{../img/Color.png}
            \caption{Color}
        \end{subfigure}\hfill
        \begin{subfigure}{0.32\textwidth}
            \includegraphics[trim={5cm 0 5cm 0},clip,width=\linewidth]{../img/Angled.png}
            \caption{Rotated}
        \end{subfigure}
        \caption{The three different designs in the same error state.
        (b) The color feedback has been activated here, changing the guidance target from green to red.}
        \label{fig:designs}%
    \end{center}
\end{figure}

\begin{figure}[tb!]
    \begin{center}
        % Thanks
        % https://tex.stackexchange.com/questions/67573/tikz-shift-and-rotate-in-3d

        \newcommand{\rotateRPY}[3]% roll, pitch, yaw
        {   \pgfmathsetmacro{\rollangle}{#1}
            \pgfmathsetmacro{\pitchangle}{#2}
            \pgfmathsetmacro{\yawangle}{#3}

            % to what vector is the x unit vector transformed, and which 2D vector is this?
            \pgfmathsetmacro{\newxx}{cos(\yawangle)*cos(\pitchangle)}
            \pgfmathsetmacro{\newxy}{sin(\yawangle)*cos(\pitchangle)}
            \pgfmathsetmacro{\newxz}{-sin(\pitchangle)}
            \path (\newxx,\newxy,\newxz);
            \pgfgetlastxy{\nxx}{\nxy};

            % to what vector is the y unit vector transformed, and which 2D vector is this?
            \pgfmathsetmacro{\newyx}{cos(\yawangle)*sin(\pitchangle)*sin(\rollangle)-sin(\yawangle)*cos(\rollangle)}
            \pgfmathsetmacro{\newyy}{sin(\yawangle)*sin(\pitchangle)*sin(\rollangle)+ cos(\yawangle)*cos(\rollangle)}
            \pgfmathsetmacro{\newyz}{cos(\pitchangle)*sin(\rollangle)}
            \path (\newyx,\newyy,\newyz);
            \pgfgetlastxy{\nyx}{\nyy};

            % to what vector is the z unit vector transformed, and which 2D vector is this?
            \pgfmathsetmacro{\newzx}{cos(\yawangle)*sin(\pitchangle)*cos(\rollangle)+ sin(\yawangle)*sin(\rollangle)}
            \pgfmathsetmacro{\newzy}{sin(\yawangle)*sin(\pitchangle)*cos(\rollangle)-cos(\yawangle)*sin(\rollangle)}
            \pgfmathsetmacro{\newzz}{cos(\pitchangle)*cos(\rollangle)}
            \path (\newzx,\newzy,\newzz);
            \pgfgetlastxy{\nzx}{\nzy};
        }

        \tikzset{RPY/.style={x={(\nxx,\nxy)},y={(\nyx,\nyy)},z={(\nzx,\nzy)}}}

        \begin{tikzpicture}
            \draw[-] node at (3.5,0,0) {x} (-3,0,0) -- (3,0,0);
            \draw[-] node at (0,3.5,0) {y, {\color{red} y\textprime}} (0,-3,0) -- (0,3,0);
            \draw[-] node at (0,0,3.5) {z} (0,0,-3) -- (0,0,3);

            \rotateRPY{0}{-30}{0}
            \begin{scope}[draw=red, text=red,fill=red,densely dashed,RPY]
                \draw[-] node at (3.5,0,0) {x\textprime} (-3,0,0) -- (3,0,0);
                %\draw[-] node at (0,3.5,0) {} (0,0,0) -- (0,3,0);
                \draw[-] node at (0,0,3.5) {z\textprime} (0,0,-3) -- (0,0,3);
            \end{scope}
            \draw [->] (0:1.5) arc (0:-15:1.5);
            \draw[-] node at (1.7,-.2,0) {$\theta$};
        \end{tikzpicture}

        \caption{Perspective display of the coordinate frame for the tracking tasks, with the $x$, $y$, and $z$ axes labeled. After rotating by $\theta$ around the y axis, the resulting reference frame of $x\textprime y\textprime z\textprime$ is also labeled.}
        \label{designdiagram}
    \end{center}
\end{figure}

% \begin{table}[tb]
% \caption{The factors that were modified between the different designs.}
% \centering
% \begin{tabular}{*{3}{r}}
% \toprule
% Design & $\theta$ (degrees) & Feedback \\
% \midrule
% Baseline & 0 & No \\
% Feedback & 0 & Yes \\
% Rotated & 30 & No \\
% \bottomrule
% \end{tabular}
% \label{tab:designs}
% \end{table}

Three designs were presented to the subjects to evaluate: a baseline design, a color-based concurrent bandwidth feedback (CBF) design, and a rotated design.
Figure~\ref{fig:designs} shows all three designs in the same error state.
The three designs were very similar, having only minor differences between each other.
Compared to the baseline, the color feedback and rotated displays were expected to produce superior performance along the $z$ axis tracking task.

The baseline design consists of a flat cross with a center target point and a green sphere error indicator.
This indicator also casts a green, variable-length rod perpendicular to the plane of the cross, which allows for a visual estimation of the error in the $z$ axis.
The $x$ axis is parallel with the horizontal cross, while the $y$ axis is parallel with the vertical cross.
The color feedback display was identical to the baseline design in every way, with the additional of visual concurrent bandwidth feedback on the $z$ axis.
When the absolute value of the error on the $z$ axis exceeded a fixed bandwidth of 0.2 units, the color of both the spherical indicator and the cylindrical rod changed from green to red.
When the absolute value of the error on the $z$ axis was lowered back below this fixed bandwidth, the indicator changed back to a green color.
The rotated display was identical to the baseline design, but the relative attitude of the display was rotated about the $y$ axis by 30 degrees, see Figure~\ref{designdiagram}.

Before entering the study, subjects were randomly placed in a display group (either the LCD monitor or HoloLens), and were then randomly placed into an order group (which consisted of Baseline-Feedback-Rotated, Feedback-Rotated-Baseline, and Rotated-Baseline-Feedback).
This order group was created to remove any order effects that might arise due to training on a given display, and follows a standard Latin squares design.
(The order of the designs was initially thought to be insignificant, but we will later discuss how this is not the case.)
% Subjects were naive to this order, and were only told which design they were evaluating directly before they began.
After entering the experiment room, the subjects were familiarized with the task, three designs, NASA-TLX, and the controller through a twenty minute training session.
Subjects were instructed that they would be evaluating three different display designs. %: a baseline display, a color feedback display, and and rotated display.
They were further instructed that they had two tasks.
They were told to:
\begin{itemize}
    \item Minimize the displacement of their guidance target from the center
    \item Respond to the two choice task as accurately and quickly as possible
\end{itemize}

After the training, subjects were seated in front of the simulator.
Subjects in the 3D group then completed a short calibration program that adjusted the display to their interpupillary distance.
After this, they were instructed on how to align the guidance cross with the center of the left display monitor.
All subjects were then allowed to complete two familiarization trials, during which time they could ask questions about how the controls worked, or any other aspects of the task.
The proctor also used this time to ensure that subjects showed basic competency with the task by responding to both the tracking and two-choice tasks appropriately.
All familiarizations were done with the baseline design, regardless of which design the subjects evaluated first.

After this familiarization process, subjects completed ten trials with their first design.
After completing these trials, they answered a brief survey which asked them if the design was adequate to complete the task.
Subjects were also asked to subjectively rate their performance in a questionnaire after evaluating each design.
Subjects were asked to rate ``I found the tracking display adequate to complete the task.'' on a five point scale where 1 indicated ``Strongly Disagree'' and 5 indicated ``Strongly Agree''.
After this survey, subjects then completed a NASA-TLX workload survey.
Subjects then repeated this process with their second and third designs.
After completing this process with their third design, subjects were also asked to complete a preference survey which inquired into what design the subjects believed to be the best.

\subsubsection{Results}
We conducted three-way mixed ANOVAs between display (2D or 3D), design (Baseline, Feedback, or Rotated), and starting design (Baseline, Feedback, or Rotated) with repeated measures on the design factor.
When significant effects were observed, post hoc comparisons using the Tukey Honest Significance Difference (HSD) test with a Bonferroni adjustment were completed to investigate which pairs of the factor were significant.
In order to remove learning and fatigue effects, each subject's best performing five trials in each design were averaged together to produce one average score for each subject and design.
Additionally, the first ten seconds of each sixty second trial were not included in the analysis to remove initial transient effects.

% \subsection{Tracking Performance}
The root-mean-square error (RMSE) of each axis was analyzed individually and combined to understand the differences between the three designs and the two devices.
% For a given variable, $i$, RMSE is calculated via
% \begin{align}
% \text{RMSE}_i = \sqrt{\dfrac{1}{n} \sum_{j=1}^{n} \left(i_j - \hat{i}_j\right)^{2}}
% \end{align}
% where $i_j$ is an observed value at time $j$ and $\hat{i}_j$ is a targeted, predicted, or otherwise desired value.
% As this is a regulation task, however, $\hat{i}_j = 0$ for all $j$.
The RMS of the disturbance signal was calculated along each axis and used to normalize the RMSE.
Under this definition, an RMSE of 1 indicates performance equivalent to a no-input response, and an RMSE greater than 1 indicates quite poor performance.
% The RMSE along the $i^{th}$ axis, RMSE$_i$, was calculated for the $x$, $y$, and $z$ axes via
% \begin{align}
% \text{RMSE}_i = \sqrt{\dfrac{1}{n} \sum_{j=1}^{n} \left(i_j\right)^{2}}
% \end{align}
It was expected that the baseline design would lead to the worst performance in the $z$ axis than the color feedback and rotated designs.
It was also expected that, due to the stereoscopic nature of the display, the HoloLens would allow for better performance than the LCD monitor along $z$ axis.

% \begin{table}[tb]
% \caption{Results of three-way ANOVA on $z$ axis RMSE.
% All main effects significant, several significant interactions.}
% \centering
% \begin{tabular}{*{5}{r}}
% \toprule
%                             & numDF & denDF & F-value   & p-value \\
% \midrule
% % (Intercept)                 & 1     & 36     & 2354.6349 & <.0001 \\
% design                      & 2     & 36     &   84.9179 & <.0001 \\
% device                      & 1     & 18     &    7.2158 & 0.0151 \\
% startdesign                 & 2     & 18     &    4.8132 & 0.0212 \\
% design:device               & 2     & 36     &    1.8359 & 0.1741 \\
% design:startdesign          & 4     & 36     &    8.5499 & 0.0001 \\
% device:startdesign          & 2     & 18     &    2.4652 & 0.1132 \\
% design:device:startdesign   & 4     & 36     &    5.5672 & 0.0014 \\
% \bottomrule
% \end{tabular}
% \label{tab:exp1anovarmse}
% \end{table}

% \begin{table}[tb]
% \caption{Results of three-way ANOVA on NASA-TLX.
% No significant effects or interactions.}
% \centering
% \begin{tabular}{*{5}{r}}
% \toprule
%                             & numDF & denDF & F-value   & p-value \\
% \midrule
% % (Intercept)                 & 1   & 36 & 472.0358 & <.0001 \\
% design                      & 2   & 36 &   0.3615 & 0.6991 \\
% device                      & 1   & 18 &   0.3797 & 0.5455 \\
% startdesign                 & 2   & 18 &   0.2769 & 0.7613 \\
% design:device               & 2   & 36 &   0.5595 & 0.5764 \\
% design:startdesign          & 4   & 36 &   0.2649 & 0.8985 \\
% device:startdesign          & 2   & 18 &   0.0025 & 0.9975 \\
% design:device:startdesign   & 4   & 36 &   0.5381 & 0.7086 \\
% \bottomrule
% \end{tabular}
% \label{tab:exp1anovaworkload}
% \end{table}

% > startdesign
%          Simultaneous Tests for General Linear Hypotheses
% Multiple Comparisons of Means: Tukey Contrasts
% Fit: lme.formula(fixed = Zrmse ~ design * device * startdesign, data = data,
%     random = ~1 | subject/design)

% Linear Hypotheses:
%            Estimate Std. Error z value Pr(>|z|)
% 2 - 1 == 0 -0.27861    0.07566  -3.683 0.000693 ***
% 3 - 1 == 0  0.11523    0.07566   1.523 0.383277
% 3 - 2 == 0  0.39383    0.07566   5.206  5.8e-07 ***
% ---
% Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
% (Adjusted p values reported -- bonferroni method)

\begin{figure}[tb!]
    \begin{center}
        \includegraphics[width=\linewidth]{./../img/x_design_y_zrmse_col_startdesign_hue_device.pdf}
        % \includegraphics[width=\linewidth]{./../img/x_design_y_zrmse_hue_startdesign_col_device.pdf}
        \caption{Three-way ANOVA $z$-axis RMSE.}
        \label{fig:zrmseanovas}
    \end{center}
\end{figure}

Results of the ANOVA on the $z$-axis RMSE showed significant effects for design ($F(2, 36)=84.9, p<.001$), device ($F(1, 18)= 7.2, p<0.015$), and start design ($F(2, 18)=4.8, p<0.021$).
The ANOVA also showed a significant interaction effect between design and starting design ($F(4, 36)=8.5499, p<0.0001$), and a three way interaction between design, device, and starting design ($F(4, 36)=5.5672, p<0.0014$).
Further investigation into the effect of starting design using a Tukey HSD comparison with a Bonferroni correction showed significance differences between subjects that started in the concurrent bandwidth feedback group and those in the baseline ($p<0.001$) or rotated ($p<0.001$) designs, but no difference between subjects that started in the baseline and rotated designs ($p>.38$).
The difference in performance along the depth axis between design, device, and starting design can be seen in Figure~\ref{fig:zrmseanovas}.

% >     y = lme(Zrmse ~ design*device, data=data, random=~1|subject/design, subset=startdesign==2)
% >     anova <- anova(y)
% > anova
%               numDF denDF   F-value p-value
% (Intercept)       1    12 2665.7021  <.0001
% design            2    12   21.6481  0.0001
% device            1     6   36.7320  0.0009
% design:device     2    12    5.4211  0.0210
%
% >     y = lme(Zrmse ~ design*device, data=data, random=~1|subject/design, subset=startdesign!=2)
% >     anova <- anova(y)
% > anova
%               numDF denDF   F-value p-value
% (Intercept)       1    28 1285.5677  <.0001
% design            2    28   38.6980  <.0001
% device            1    14    1.0299  0.3274
% design:device     2    28    0.0263  0.9741

Due to the presence of interaction effects in the starting design factor, the remainder of the analysis is split between subjects who started with the concurrent bandwidth feedback design and those that did not (e.g., those that started in the baseline or rotated design).
For subjects that started in the CBF design, there was a significant effect of design ($F(2, 12)= 21.6481, p <.0001$), a significant effect of display ($F(1, 6)=  36.7320, p <0.001$), and a significant interaction effect between design and device ($F(2, 12)=  5.4211, p <0.021$).
For subjects that did not start in the CBF design, there was a significant effect of design ($F(2, 28)= 38.6980, p <.0001$), no significant effect or display ($F(1, 14)=  1.0299, p >0.32$), and no interaction effect ($F(2, 12)=  0.0263, p >0.97$).
The resulting difference between subjects who started in the CBF design and those that did not is presented in Figure~\ref{fig:zrmseanovas2}.
For subjects that did not start in the CBF design, display had no significant effect, and subjects performed best in the rotated design, followed by the feedback design and finally the baseline design.
Subjects that started in the CBF design performed significantly better in the 3D display than the 2D displays, and performed best in the rotated design, but comparably between the feedback and baseline designs.

\begin{figure}[tb!]
    \begin{center}
        \includegraphics[width=0.8\linewidth]{./../img/x_design_y_zrmse_hue_device_col_cbf_first.pdf}
        \caption{Three-way ANOVA $z$-axis RMSE, grouping by subjects that started with or without feedback.}
        \label{fig:zrmseanovas2}
    \end{center}
\end{figure}

% >     y = lme(rt ~ design*device*startdesign, data=data, random=~1|subject/design)
% >     anova <- anova(y)
% > anova
%                           numDF denDF   F-value p-value
% (Intercept)                   1    36 1290.3273  <.0001
% design                        2    36    7.9333  0.0014
% device                        1    18    1.2139  0.2851
% startdesign                   2    18    2.0733  0.1548
% design:device                 2    36    0.0045  0.9955
% design:startdesign            4    36    1.1689  0.3408
% device:startdesign            2    18    1.1634  0.3348
% design:device:startdesign     4    36    0.1799  0.9473
% >     y = lme(Score ~ design*device*startdesign, data=data, random=~1|subject/design)
% >     anova <- anova(y)
% > anova
%                           numDF denDF  F-value p-value
% (Intercept)                   1    36 472.0358  <.0001
% design                        2    36   0.3615  0.6991
% device                        1    18   0.3797  0.5455
% startdesign                   2    18   0.2769  0.7613
% design:device                 2    36   0.5595  0.5764
% design:startdesign            4    36   0.2649  0.8985
% device:startdesign            2    18   0.0025  0.9975
% design:device:startdesign     4    36   0.5381  0.7086

The NASA-TLX was used to measure differences in subjective workload, and the reaction time to the secondary task was used to measure differences in objective workload between design, device, and starting design.
There were no significant effects, nor interaction effects, found in the ANOVA for design, device, or starting design for the NASA-TLX measurements.
There was a significant effect of design ($F(2, 36)=  7.9333, p <0.0014$) for the reaction time to the secondary task, though the magnitude of this effect was very small between designs (less than 100 ms difference) and was not significant during Tukey HSD tests.
In general, there were no significant effects found for workload measurements.

To summarize these results, there were significant effects found in the $z$-axis RMSE for design, with subjects generally performing the best using the angled design, followed by the CBF design, and performing worst with the baseline design.
There were significant effects found for the factors of device and starting design, though these must be interpreted carefully.
Subjects that started in the design with concurrent bandwidth feedback performed better than subjects that did not.
We believe that this result resembles that found in our prior SAFER experiment, where subjects that were exposed to the CBF early on learned the task better than those that were not exposed.
An interesting effect of this exposure is that, after learning the task with CBF, subjects continued on to perform significantly better in the baseline condition than those subjects that did not start in the CBF design.

Subjects that started in the CBF design and that were wearing the HoloLens appear to have used the CBF to better learn the depth cue presented in the stereoscopic display.
These subjects continued to perform significantly better than subjects who started with the CBF design but without the stereoscopic display when they continued to the baseline design.
This indicates that even a brief exposure to the concurrent bandwidth feedback was sufficient to induce improved performance in the baseline design.
Additionally, this also suggests that well trained subjects could perform better using the stereoscopic display compared with the traditional display, but that subjects who were still learning the task could not take advantage of the additional depth cues provided by the display.
Finally, there were no significant effects found for the NASA-TLX measurements, and there were significant effects found between the designs for reaction time, though this difference was very small at less than 100 ms.

In summary, we find partial agreement with all of our hypotheses in respect to the performance aspects of our experiment, while the workload was essentially unaffected by all of our experimental factors.
For Hypothesis 1, subjects that completed the baseline design before the CBF design performed better in the CBF design, while subjects that completed the CBF design before the baseline performed approximately the same in both designs.
This indicates that CBF can both improve performance compared to a baseline design, and better train subjects such that, even after brief exposure, the feedback is no longer required.
Workload was unaffected by the CBF.
For Hypothesis 2, subjects that started in the CBF design appear to have better learned the task and used the CBF to learn to interpret the depth cue provided by the stereoscopic display.
Subjects that were not initially exposed to this feedback were unable to exploit the display, and did not perform significantly better than subjects without the display.
For Hypothesis 3, subjects in the rotated design did perform better than those in the baseline design and appeared to be able to use this view to better interpret the depth of the target.
An unexpected side effect of angling the display, however, was that subjects performed worse in the $x$ axis tracking task, which was rotated out of parallel with the display.

\subsection{Experiment Two}
\begin{figure}[tb]
    \begin{center}
        \includegraphics[trim={13cm 5cm 22cm 15.5cm},clip,width=\linewidth]{./../img/Screen Shot 2018-07-26 at 1.43.16 PM.png}
        \caption{The Robotic On-board Trainer (ROBoT) station set up in the NASA HERA Analog, from~\cite{robottalk}}
        \label{figure:robotinhera}
    \end{center}
\end{figure}

The second Aim of the proposal is addressed by Experiment Two.
Experiment Two will investigate if concurrent bandwidth feedback can decrease the required learning time to peak performance in a simulated robotic arm track and capture task.
We will compare task performance and workload between two groups: a control group, which receives no feedback, and a treatment group, which receives concurrent bandwidth feedback on one or more sensor readouts.
Subjects in both groups will complete the same task, and it is hypothesized that subjects in the treatment group will perform the task better and require less training time to reach peak performance.
This hypothesis is based on the results of the SAFER experiment, as well as the results of Experiment One.

In this task, subjects will command a robotic arm to track an approaching vehicle and grapple with it.
This is motivated by the primary use of the robot arm on the International Space Station, which grasps visiting vehicles when they arrive at the station and then attaches them to a separate fixed location on the station.
Subjects will be trained on this task and will then repeat the task for one to two hours through a variety of slightly different start and end conditions.
% While they are completing the robotics task, subjects will also be required to attend to a secondary task which will require them to look away from their primary flight display.
% Subjects will also be asked to report their subjective workload after each trial.

NASA’s Robotic On-board Trainer (ROBoT) will be used for this experiment.
ROBoT is a package of simulation software which includes a dynamic model of the robotic arm on the space station and presents the user with multiple camera angle views and the instrument panel required to operate effectively, see Figure~\ref{figure:robotinhera}.
In addition to these displays, ROBoT also includes the two hand controllers required to control the arm.
NASA trainers at Johnson Space Center developed metrics which are included in ROBoT and include the time to capture, the alignment measurements during approach, the amount of wobble in the arm, the number of times the grapple fixture contacted the structure, the overall path efficiency, and the number of capture attempts.
The preceding list presents candidate metrics from which we may observe human performance.

\subsubsection{Hypotheses}
This study will assess the influence of concurrent bandwidth feedback (with vs. without) on performance and workload.
Objective performance data will be measured using time to capture, alignment error metrics, and other metrics available by the ROBoT system.
Subjective workload will be measured using the NASA-TLX at several points throughout the experiment.
It is hypothesized that:
\begin{description}[align=left]
\item [Hypothesis 1] Concurrent bandwidth feedback will improve performance in the track-and-capture task.
\item [Hypothesis 2] Concurrent bandwidth feedback will cause subjects to more quickly reach their peak performance in the track-and-capture task.
\item [Hypothesis 3] Concurrent bandwidth feedback will decrease workload in the track-and-capture task.
\end{description}

\subsubsection{Previous Work}
We have experience using the ROBoT simulation software from participating in a study investigating the effects of sleep loss and circadian misalignment on performance on the ROBoT simulator at NASA Ames~\cite{robotreport}.
In this study, subjects trained during one-hour long sessions each day for five days, then spent a twenty four hour period in the lab while they continued to perform robotic tasks.
We did not find evidence of performance loss during sleep deprivation or circadian misalignment on any of the ROBoT performance metrics.
In fact, participants continued to show improvement over time, which indicated that they had continued to learn the task despite the sleep loss~\cite{robotreport}.
This finding reinforces the need for enhanced learning techniques, as the current training strategy requires a tremendous amount of time to reach peak performance.

% \begin{figure}[tb!]
%     \begin{center}
%         \includegraphics[trim={13cm 5cm 22cm 15.5cm},clip,width=\linewidth]{./../img/Screen Shot 2018-07-26 at 1.43.02 PM.png}
%         \caption{ROBoT visualization laptop, showing four camera views.}
%         % \label{}
%     \end{center}
% \end{figure}

% \begin{figure}[tb!]
%     \begin{center}
%         \includegraphics[trim={13cm 5cm 22cm 15.5cm},clip,width=\linewidth]{./../img/Screen Shot 2018-07-26 at 1.43.05 PM.png}
%         \caption{The camera attached to the end effector of the robotic arm, showing the grapple fixture.}
%         % \label{}
%     \end{center}
% \end{figure}

% \begin{figure}[tb!]
%     \begin{center}
%         \includegraphics[trim={13cm 5cm 22cm 15.5cm},clip,width=\linewidth]{./../img/Screen Shot 2018-07-26 at 1.43.07 PM.png}
%         \caption{Example performance score report shown to the user after each trial.}
%         % \label{}
%     \end{center}
% \end{figure}

% \begin{figure}[tb!]
%     \begin{center}
%         \includegraphics[trim={13cm 5cm 22cm 15.5cm},clip,width=\linewidth]{./../img/Screen Shot 2018-07-26 at 1.43.10 PM.png}
%         \caption{The hand controller inputs and position and angular errors are also logged throughout the trial.}
%         % \label{}
%     \end{center}
% \end{figure}

\subsection{Model Extension}
The third and final Aim of the proposal is addressed by developing a model of the human pilot which includes the effects of concurrent bandwidth feedback.
The proposed Model will extend Professor Hess’ structural model of the human pilot to include the effects of concurrent bandwidth feedback.
% The outputs of this model will be compared to the results of Experiment One and Two.
The Structural Model has been extremely successful in predicting human performance through a variety of system dynamics and can predict how performance changes during a pilot's adaptation to changing dynamics.
Hess has developed adaptive logic for the human pilot in a pursuit task which triggers when the pilot notices that vehicle dynamics have changed~\cite{Hess2009}.
This logic is based off several criteria, which include ``the logic driving the adjustments... must be predicated upon information available to the human'' and ``the postadapated pilot models must follow the dictates of the crossover model of the human pilot~\cite{Hess2009}.''
The primary result of the adaptive logic is to increase the resulting crossover frequency of the pilot, effectively making them more aggressive or implying more focus on the task.
Our initial approach to adding concurrent bandwidth feedback into the Structural Model will be based off of Hess' approach to modeling human adaptation in pursuit tasks, which is currently ad-hoc in nature, see Figure~\ref{figure:hesspursuit}.

\begin{figure}[tb]
    \begin{center}
        \includegraphics[width=0.8\linewidth]{./../img/Screen Shot 2018-08-09 at 4.15.24 PM.png}
        \caption{Hess' model of the adaptive human pilot, from~\cite{Hess2009}}
        \label{figure:hesspursuit}
    \end{center}
\end{figure}

While this model of the adaptive pilot has been successful in predicting changes in performance for a well trained subject, it does not consider how a pilot would behave when they are still in the early stages of training.
Our modified model will include two major changes to Hess' current model:
\begin{itemize}
\item The adaptation logic will be changed to focus on concurrent bandwidth feedback
\item The timescale of the adaptation will be significantly longer
% \item We plan to use the full structural model presented in Figure~\ref{figure:structuralmodel}, rather than the simplified pursuit model presented in Figure~\ref{figure:hesspursuit}
\end{itemize}

We propose to modify the adaptive logic to trigger when the pilot is receiving concurrent bandwidth feedback, rather than when a change in system dynamics occurs.
This will require the addition of a feedback loop onto the Structural Model which triggers when the bandwidth feedback is activated.
This loop will likely be based around the $K_e$ gain, which is currently the primary way of setting the crossover frequency in the Structural Model.
This implies that the subjects in our experiments do their primary learning when they are receiving qualitative feedback that their current level of aggressiveness is not sufficient to complete the task.
While there a separate loop that adjusts the crossover frequency as learning progresses over the course of several hours clearly exists, the change in performance we see when subjects use the concurrent bandwidth feedback happens relatively rapidly, within a few minutes.
This is reflected in the delta of performance between subjects in the different groups of our SAFER experiment, even on the first trial, see Figure~\ref{figure:saferdistance}.
This relates to the second required change, the amount the required adaptation time.
Professor Hess' model requires that pilots adapt within a very short time period, on the order of 5 seconds~\cite{weir1966}.
The results of our experiment with SAFER and the three-axis tracking task also suggest relatively short adaptation times, though they are on the order of a few minutes, again see Figure~\ref{figure:saferdistance}.

% Finally, there are many advantages to using the full structural model over the simplified pursuit model.
% While we have only shown subjects visual concurrent bandwidth feedback, we could also provide haptic feedback on the stick or seat.
% While all of our experiments have taken place in fixed based simulators, it is tempting to consider how the effects of our feedback interact with the effects of motion based simulators.

While it should be noted that this work is in very preliminary stages and not scheduled to begin until the Fall Quarter, some early work has been started in preparation for the qualifying examination.
An effort has been made to begin to replicate some of Hess' results.
Working with Professor Hess, we have been able to replicate some of the existing adaptation logic for a two-axis tracking task, and begun exploratory research into modifying the model.

\end{document}
